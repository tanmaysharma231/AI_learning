{"cells":[{"source":"# Building Multimodal AI Applications with LangChain & the OpenAI API ","metadata":{},"id":"bbf7941b-6c0a-4b14-b1fc-c703e57e352b","cell_type":"markdown"},{"source":"## Goals ","metadata":{},"id":"333b6c1a-a3c4-4c3c-b5e1-145bd214f4e0","cell_type":"markdown"},{"source":"Videos can be full of useful information, but getting hold of that info can be slow, since you need to watch the whole thing or try skipping through it. It can be much faster to use a bot to ask questions about the contents of the transcript.\n\nIn this project, you'll download a tutorial video from YouTube, transcribe the audio, and create a simple Q&A bot to ask questions about the content.","metadata":{},"id":"701b76fe-04db-405c-98f7-f0f5babd84b4","cell_type":"markdown"},{"source":"### Maintenance note, May 2024\n\nSince this code-along was released, the Python packages for working with the OpenAI API have changed their syntax. The instructions, hints, and code have been updated to use the latest syntax, but the video has not been updated. Consequently, it is now slightly out of sync. Trust the workbook, not the video.","metadata":{},"id":"7ae91e65-e94a-4858-b7b4-2316499f489d","cell_type":"markdown"},{"source":"- Understanding the building blocks of working with Multimodal AI projects\n- Working with some of the fundamental concepts of LangChain  \n- How to use the Whisper API to transcribe audio to text \n- How to combine both LangChain and Whisper API to create ask questions of any YouTube video ","metadata":{},"id":"3e302e1c-4c18-4c44-87fd-ba935c3a0853","cell_type":"markdown"},{"source":"## Before you begin","metadata":{},"id":"8231d2c6-275e-4399-b7cd-84e112831d08","cell_type":"markdown"},{"source":"You'll need a developer account with [OpenAI ](https://auth0.openai.com/u/signup/identifier?state=hKFo2SAyeTZBU1pzbUNWYWs3Wml5OWVvUVh4enZldC1LYU9PMaFur3VuaXZlcnNhbC1sb2dpbqN0aWTZIDFUakNoUGFMLUdNWFpfQkpqdncyZjVDQk9xUTE4U0xDo2NpZNkgRFJpdnNubTJNdTQyVDNLT3BxZHR3QjNOWXZpSFl6d0Q) and a create API Key. The API secret key will be stored in your 'Environment Variables' on the side menu. See the *getting-started.ipynb* notebook for details on setting this up.","metadata":{},"id":"785d7fac-edb2-482f-be2b-c63dc2882103","cell_type":"markdown"},{"source":"## Task 0: Setup","metadata":{},"id":"a9274661-8d8c-4cc5-901e-5fc497866b89","cell_type":"markdown"},{"source":"The project requires several packages that need to be installed into Workspace.\n\n- `langchain` is a framework for developing generative AI applications.\n- `yt_dlp` lets you download YouTube videos.\n- `tiktoken` converts text into tokens.\n- `docarray` makes it easier to work with multi-model data (in this case mixing audio and text).","metadata":{},"id":"823598ac-fa77-4532-997d-2923d0017e90","cell_type":"markdown"},{"source":"### Instructions\n\nRun the following code to install the packages.","metadata":{},"id":"c17ab340-c582-4ba7-ab33-5d582210f5c2","cell_type":"markdown"},{"source":"# Install the openai package, locked to version 1.27\n!pip install openai==1.27\n\n# Install the langchain package, locked to version 0.1.19\n!pip install langchain==0.1.19\n\n# Install the langchain-openai package, locked to version 0.1.6\n!pip install langchain-openai==0.1.6\n\n# Install the yt_dlp package, locked to version 2024.4.9\n!pip install yt_dlp==2024.4.9\n\n# Install the tiktoken package, locked to version 0.6.0\n!pip install tiktoken==0.6.0\n\n# Install the docarray package, locked to version 0.40.0\n!pip install docarray==0.40.0","metadata":{"executionCancelledAt":null,"executionTime":18988,"lastExecutedAt":1747713850226,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Install the openai package, locked to version 1.27\n!pip install openai==1.27\n\n# Install the langchain package, locked to version 0.1.19\n!pip install langchain==0.1.19\n\n# Install the langchain-openai package, locked to version 0.1.6\n!pip install langchain-openai==0.1.6\n\n# Install the yt_dlp package, locked to version 2024.4.9\n!pip install yt_dlp==2024.4.9\n\n# Install the tiktoken package, locked to version 0.6.0\n!pip install tiktoken==0.6.0\n\n# Install the docarray package, locked to version 0.40.0\n!pip install docarray==0.40.0","outputsMetadata":{"0":{"height":616,"type":"stream"}},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":false},"lastExecutedByKernel":"71484f49-fabc-44a9-804f-e00db453715b"},"id":"e1ca41e3-2dfd-4b9a-b595-e5af720ca36a","cell_type":"code","execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":"Defaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: openai==1.27 in /home/repl/.local/lib/python3.8/site-packages (1.27.0)\nRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.8/dist-packages (from openai==1.27) (3.6.1)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.8/dist-packages (from openai==1.27) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from openai==1.27) (0.27.0)\nRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from openai==1.27) (1.10.12)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.8/dist-packages (from openai==1.27) (1.2.0)\nRequirement already satisfied: tqdm>4 in /usr/local/lib/python3.8/dist-packages (from openai==1.27) (4.64.0)\nRequirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.8/dist-packages (from openai==1.27) (4.12.2)\nRequirement already satisfied: idna>=2.8 in /usr/lib/python3/dist-packages (from anyio<5,>=3.5.0->openai==1.27) (2.8)\nRequirement already satisfied: certifi in /usr/lib/python3/dist-packages (from httpx<1,>=0.23.0->openai==1.27) (2019.11.28)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.8/dist-packages (from httpx<1,>=0.23.0->openai==1.27) (1.0.5)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.8/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.27) (0.14.0)\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: langchain==0.1.19 in /home/repl/.local/lib/python3.8/site-packages (0.1.19)\nRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.8/dist-packages (from langchain==0.1.19) (6.0.1)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.8/dist-packages (from langchain==0.1.19) (1.4.40)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/repl/.local/lib/python3.8/site-packages (from langchain==0.1.19) (3.10.11)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from langchain==0.1.19) (4.0.2)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/repl/.local/lib/python3.8/site-packages (from langchain==0.1.19) (0.6.7)\nRequirement already satisfied: langchain-community<0.1,>=0.0.38 in /home/repl/.local/lib/python3.8/site-packages (from langchain==0.1.19) (0.0.38)\nRequirement already satisfied: langchain-core<0.2.0,>=0.1.52 in /home/repl/.local/lib/python3.8/site-packages (from langchain==0.1.19) (0.1.53)\nRequirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /home/repl/.local/lib/python3.8/site-packages (from langchain==0.1.19) (0.0.2)\nRequirement already satisfied: langsmith<0.2.0,>=0.1.17 in /home/repl/.local/lib/python3.8/site-packages (from langchain==0.1.19) (0.1.147)\nRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.8/dist-packages (from langchain==0.1.19) (1.23.2)\nRequirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.8/dist-packages (from langchain==0.1.19) (1.10.12)\nRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.8/dist-packages (from langchain==0.1.19) (2.31.0)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from langchain==0.1.19) (8.2.3)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/repl/.local/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.19) (2.4.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.19) (1.2.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.19) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.19) (1.3.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.19) (6.0.2)\nRequirement already satisfied: yarl<2.0,>=1.12.0 in /home/repl/.local/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.19) (1.15.2)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/repl/.local/lib/python3.8/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.19) (3.22.0)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.19) (0.9.0)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /home/repl/.local/lib/python3.8/site-packages (from langchain-core<0.2.0,>=0.1.52->langchain==0.1.19) (1.33)\nRequirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.8/dist-packages (from langchain-core<0.2.0,>=0.1.52->langchain==0.1.19) (23.2)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.1.19) (0.27.0)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/repl/.local/lib/python3.8/site-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.1.19) (3.10.15)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /home/repl/.local/lib/python3.8/site-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.1.19) (1.0.0)\nRequirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic<3,>=1->langchain==0.1.19) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2->langchain==0.1.19) (2.0.12)\nRequirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2->langchain==0.1.19) (2.8)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /home/repl/.local/lib/python3.8/site-packages (from requests<3,>=2->langchain==0.1.19) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2->langchain==0.1.19) (2019.11.28)\nRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.8/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.1.19) (1.1.3)\nRequirement already satisfied: anyio in /usr/local/lib/python3.8/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.19) (3.6.1)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.8/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.19) (1.0.5)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.8/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.19) (1.2.0)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.8/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.19) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.8/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.52->langchain==0.1.19) (2.4)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.19) (0.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /home/repl/.local/lib/python3.8/site-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain==0.1.19) (0.2.0)\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: langchain-openai==0.1.6 in /home/repl/.local/lib/python3.8/site-packages (0.1.6)\nRequirement already satisfied: langchain-core<0.2.0,>=0.1.46 in /home/repl/.local/lib/python3.8/site-packages (from langchain-openai==0.1.6) (0.1.53)\nRequirement already satisfied: openai<2.0.0,>=1.24.0 in /home/repl/.local/lib/python3.8/site-packages (from langchain-openai==0.1.6) (1.27.0)\nRequirement already satisfied: tiktoken<1,>=0.5.2 in /home/repl/.local/lib/python3.8/site-packages (from langchain-openai==0.1.6) (0.6.0)\nRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.8/dist-packages (from langchain-core<0.2.0,>=0.1.46->langchain-openai==0.1.6) (6.0.1)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /home/repl/.local/lib/python3.8/site-packages (from langchain-core<0.2.0,>=0.1.46->langchain-openai==0.1.6) (1.33)\nRequirement already satisfied: langsmith<0.2.0,>=0.1.0 in /home/repl/.local/lib/python3.8/site-packages (from langchain-core<0.2.0,>=0.1.46->langchain-openai==0.1.6) (0.1.147)\nRequirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.8/dist-packages (from langchain-core<0.2.0,>=0.1.46->langchain-openai==0.1.6) (23.2)\nRequirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.8/dist-packages (from langchain-core<0.2.0,>=0.1.46->langchain-openai==0.1.6) (1.10.12)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from langchain-core<0.2.0,>=0.1.46->langchain-openai==0.1.6) (8.2.3)\nRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.8/dist-packages (from openai<2.0.0,>=1.24.0->langchain-openai==0.1.6) (3.6.1)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.8/dist-packages (from openai<2.0.0,>=1.24.0->langchain-openai==0.1.6) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from openai<2.0.0,>=1.24.0->langchain-openai==0.1.6) (0.27.0)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.8/dist-packages (from openai<2.0.0,>=1.24.0->langchain-openai==0.1.6) (1.2.0)\nRequirement already satisfied: tqdm>4 in /usr/local/lib/python3.8/dist-packages (from openai<2.0.0,>=1.24.0->langchain-openai==0.1.6) (4.64.0)\nRequirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.8/dist-packages (from openai<2.0.0,>=1.24.0->langchain-openai==0.1.6) (4.12.2)\nRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.8/dist-packages (from tiktoken<1,>=0.5.2->langchain-openai==0.1.6) (2022.8.17)\nRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.8/dist-packages (from tiktoken<1,>=0.5.2->langchain-openai==0.1.6) (2.31.0)\nRequirement already satisfied: idna>=2.8 in /usr/lib/python3/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.24.0->langchain-openai==0.1.6) (2.8)\nRequirement already satisfied: certifi in /usr/lib/python3/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain-openai==0.1.6) (2019.11.28)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.8/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain-openai==0.1.6) (1.0.5)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.8/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain-openai==0.1.6) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.8/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.46->langchain-openai==0.1.6) (2.4)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/repl/.local/lib/python3.8/site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.46->langchain-openai==0.1.6) (3.10.15)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /home/repl/.local/lib/python3.8/site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.46->langchain-openai==0.1.6) (1.0.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.5.2->langchain-openai==0.1.6) (2.0.12)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /home/repl/.local/lib/python3.8/site-packages (from requests>=2.26.0->tiktoken<1,>=0.5.2->langchain-openai==0.1.6) (2.2.3)\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: yt_dlp==2024.4.9 in /home/repl/.local/lib/python3.8/site-packages (2024.4.9)\nRequirement already satisfied: brotli in /home/repl/.local/lib/python3.8/site-packages (from yt_dlp==2024.4.9) (1.1.0)\nRequirement already satisfied: certifi in /usr/lib/python3/dist-packages (from yt_dlp==2024.4.9) (2019.11.28)\nRequirement already satisfied: mutagen in /home/repl/.local/lib/python3.8/site-packages (from yt_dlp==2024.4.9) (1.47.0)\nRequirement already satisfied: pycryptodomex in /home/repl/.local/lib/python3.8/site-packages (from yt_dlp==2024.4.9) (3.23.0)\nRequirement already satisfied: requests<3,>=2.31.0 in /usr/local/lib/python3.8/dist-packages (from yt_dlp==2024.4.9) (2.31.0)\nRequirement already satisfied: urllib3<3,>=1.26.17 in /home/repl/.local/lib/python3.8/site-packages (from yt_dlp==2024.4.9) (2.2.3)\nRequirement already satisfied: websockets>=12.0 in /home/repl/.local/lib/python3.8/site-packages (from yt_dlp==2024.4.9) (13.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.31.0->yt_dlp==2024.4.9) (2.0.12)\nRequirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.31.0->yt_dlp==2024.4.9) (2.8)\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: tiktoken==0.6.0 in /home/repl/.local/lib/python3.8/site-packages (0.6.0)\nRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.8/dist-packages (from tiktoken==0.6.0) (2022.8.17)\nRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.8/dist-packages (from tiktoken==0.6.0) (2.31.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.26.0->tiktoken==0.6.0) (2.0.12)\nRequirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.26.0->tiktoken==0.6.0) (2.8)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /home/repl/.local/lib/python3.8/site-packages (from requests>=2.26.0->tiktoken==0.6.0) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.26.0->tiktoken==0.6.0) (2019.11.28)\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: docarray==0.40.0 in /home/repl/.local/lib/python3.8/site-packages (0.40.0)\nRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from docarray==0.40.0) (1.23.2)\nRequirement already satisfied: orjson>=3.8.2 in /home/repl/.local/lib/python3.8/site-packages (from docarray==0.40.0) (3.10.15)\nRequirement already satisfied: pydantic>=1.10.8 in /usr/local/lib/python3.8/dist-packages (from docarray==0.40.0) (1.10.12)\nRequirement already satisfied: rich>=13.1.0 in /home/repl/.local/lib/python3.8/site-packages (from docarray==0.40.0) (14.0.0)\nRequirement already satisfied: types-requests>=2.28.11.6 in /home/repl/.local/lib/python3.8/site-packages (from docarray==0.40.0) (2.32.0.20241016)\nRequirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from docarray==0.40.0) (0.9.0)\nRequirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic>=1.10.8->docarray==0.40.0) (4.12.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.8/dist-packages (from rich>=13.1.0->docarray==0.40.0) (2.2.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from rich>=13.1.0->docarray==0.40.0) (2.13.0)\nRequirement already satisfied: urllib3>=2 in /home/repl/.local/lib/python3.8/site-packages (from types-requests>=2.28.11.6->docarray==0.40.0) (2.2.3)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from typing-inspect>=0.8.0->docarray==0.40.0) (0.4.3)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.8/dist-packages (from markdown-it-py>=2.2.0->rich>=13.1.0->docarray==0.40.0) (0.1.2)\n"}]},{"source":"### Instructions","metadata":{},"id":"92a9caca-70fd-4ac0-aa15-1bee55c456d3","cell_type":"markdown"},{"source":"## Task 1: Import The Required Libraries ","metadata":{},"id":"7f6c51c7-bbbb-4f0f-b218-850221f3dcdf","cell_type":"markdown"},{"source":"For this project we need the `os` and the `yt_dlp` packages to download the YouTube video of your choosing, convert it to an `.mp3` and save the file. We will also be using the `openai` package to make easy calls to the OpenAI models we will use. ","metadata":{},"id":"2cf847fd-f8f8-49f6-9b43-0eb098239072","cell_type":"markdown"},{"source":"### Instructions\n\nImport the following packages.\n\n- Import `os`. \n- Import `glob`.\n- Import `openai`.\n- Import `yt_dlp` with the alias `youtube_dl`.\n- From the `yt_dlp` package, import `DowloadError`.\n- Assign `openai_api_key` to `os.getenv(\"OPENAI_API_KEY\")`.","metadata":{},"id":"b1fcd794-b29c-4010-8be0-651a452b2044","cell_type":"markdown"},{"source":"# Import the os package\nimport os\n\n# Import the glob package\nimport glob\n\n# Import the openai package \nimport openai\n\n# Import the yt_dlp package as youtube_dl\nimport yt_dlp as youtube_dl\n\n# Import DownloadError from yt_dlp\nfrom yt_dlp import DownloadError\n\n# Import DocArray \nimport docarray\nopenai_api_key= os.getenv(\"OPENAI_API_KEY\")\n","metadata":{"executionCancelledAt":null,"executionTime":54,"lastExecutedAt":1747713850282,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import the os package\nimport os\n\n# Import the glob package\nimport glob\n\n# Import the openai package \nimport openai\n\n# Import the yt_dlp package as youtube_dl\nimport yt_dlp as youtube_dl\n\n# Import DownloadError from yt_dlp\nfrom yt_dlp import DownloadError\n\n# Import DocArray \nimport docarray\nopenai_api_key= os.getenv(\"OPENAI_API_KEY\")\n","outputsMetadata":{"0":{"height":77,"type":"stream"}},"lastExecutedByKernel":"71484f49-fabc-44a9-804f-e00db453715b"},"id":"541cd9f5-0aaa-4374-8411-25bedecd8c84","cell_type":"code","execution_count":8,"outputs":[]},{"source":"We will also assign the variable `openai_api_key` to the environment variable \"OPEN_AI_KEY\". This will help keep our key secure and remove the need to write it in the code here. ","metadata":{},"id":"794e2ce2-ba13-446f-9ac7-2b5743f65a51","cell_type":"markdown"},{"source":"openai_api_key = os.getenv(\"OPENAI_API_KEY\")","metadata":{"executionCancelledAt":null,"executionTime":46,"lastExecutedAt":1747713850330,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"openai_api_key = os.getenv(\"OPENAI_API_KEY\")","lastExecutedByKernel":"71484f49-fabc-44a9-804f-e00db453715b"},"id":"7156b205-f844-4d9e-8867-449ff5840839","cell_type":"code","execution_count":9,"outputs":[]},{"source":"## Task 2: Download the YouTube Video","metadata":{},"id":"751b9539-cbf7-4d6e-9634-045345cf8a4a","cell_type":"markdown"},{"source":"After creating the setup, the first step we will need to do is download the video from Youtube and convert it to an audio file (.mp3). \n\nWe'll download a DataCamp tutorial about machine learning in Python.\n\nWe will do this by setting a variable to store the `youtube_url` and the `output_dir` that we want the file to be stored. \n\nThe `yt_dlp` allows us to download and convert in a few steps but does require a few configuration steps. This code is provided to you. \n\nLastly, we will create a loop that looks in the `output_dir` to find any .mp3 files. Then we will store those in a list called `audio_files` that will be used later to send each file to the Whisper model for transcription. ","metadata":{},"id":"48abc459-48e5-4c7c-a795-daaf347ceef6","cell_type":"markdown"},{"source":"### Instructions\n\n- Run the code to set the URL of the video, `youtube_url`, the directory to store the downloaded video, `youtube_url`, and the download settings, `ydl_config`.\n\n_This code can be adapted to any video you choose! Just change the URL._","metadata":{},"id":"8f2f2698-f768-4437-8e7f-c11327d3d4a7","cell_type":"markdown"},{"source":"# An example YouTube tutorial video\nyoutube_url = \"https://www.youtube.com/watch?v=aqzxYofJ_ck\"\n\n# Directory to store the downloaded video\noutput_dir = \"files/audio/\"\n\n# Config for youtube-dl\nydl_config = {\n    \"format\": \"bestaudio/best\",\n    \"postprocessors\": [\n        {\n            \"key\": \"FFmpegExtractAudio\",\n            \"preferredcodec\": \"mp3\",\n            \"preferredquality\": \"192\",\n        }\n    ],\n    \"outtmpl\": os.path.join(output_dir, \"%(title)s.%(ext)s\"),\n    \"verbose\": True\n}\n","metadata":{"executionCancelledAt":null,"executionTime":51,"lastExecutedAt":1747713850382,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# An example YouTube tutorial video\nyoutube_url = \"https://www.youtube.com/watch?v=aqzxYofJ_ck\"\n\n# Directory to store the downloaded video\noutput_dir = \"files/audio/\"\n\n# Config for youtube-dl\nydl_config = {\n    \"format\": \"bestaudio/best\",\n    \"postprocessors\": [\n        {\n            \"key\": \"FFmpegExtractAudio\",\n            \"preferredcodec\": \"mp3\",\n            \"preferredquality\": \"192\",\n        }\n    ],\n    \"outtmpl\": os.path.join(output_dir, \"%(title)s.%(ext)s\"),\n    \"verbose\": True\n}\n","outputsMetadata":{"0":{"height":357,"type":"stream"},"1":{"height":137,"type":"stream"},"2":{"height":97,"type":"stream"},"3":{"height":37,"type":"stream"},"4":{"height":257,"type":"stream"},"5":{"height":77,"type":"stream"},"6":{"height":57,"type":"stream"},"7":{"height":57,"type":"stream"},"8":{"height":97,"type":"stream"},"9":{"height":77,"type":"stream"}},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":false},"lastExecutedByKernel":"71484f49-fabc-44a9-804f-e00db453715b"},"id":"ffb3836d-7b1b-47db-9ccc-6910972dd045","cell_type":"code","execution_count":10,"outputs":[]},{"source":"### Instructions\n\n- Check if `output_dir` exists, if not, then make that directory.\n- Try to download the video using the specified configuration.\n  -  If a DownloadError occurs, attempt to download the video again.","metadata":{},"id":"181a39f3-979b-411a-a53a-a7ab3a49d272","cell_type":"markdown"},{"source":"<details>\n<summary>Code hints</summary>\n<p>\n    \n- Check whether a directory exists using `os.path.exists()`.\n- Make a new directory with `os.makedirs()`.\n\n---\n    \nThe pattern for trying to do something then handling an error is:\n    \n```py\ntry:\n    # do something\nexcept TypeOfError:\n    # handle the error\n```\n\n---\n    \nThe YouTube downloader code pattern is similar to opening a file.\n\n```py\nwith youtube_dl.YoutubeDL(config_file) as ydl:\n    ydl.download([video_url])\n```\n\n</p>\n</details>","metadata":{},"id":"4fdc3d66-db63-46e2-95f9-c293be14d758","cell_type":"markdown"},{"source":"# Check if the output directory exists, if not create it\nif not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n# Print a message indicating which video is being downloaded\nprint(f\"Downloading video from {youtube_url}\")\n\n# Try to download the video using the specified configuration\n# If a DownloadError occurs, attempt to download the video again\ntry:\n    with youtube_dl.YoutubeDL(ydl_config) as ydl:\n        ydl.download([youtube_url])\nexcept DownloadError:\n     with youtube_dl.YoutubeDL(ydl_config) as ydl: \n        ydl.download([youtube_url])","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastExecutedByKernel":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":290,"type":"stream"},"1":{"height":122,"type":"stream"},"2":{"height":479,"type":"stream"},"3":{"height":101,"type":"stream"},"4":{"height":185,"type":"stream"},"5":{"height":101,"type":"stream"},"6":{"height":185,"type":"stream"}}},"id":"69ae65cf-3109-490e-8a86-a9d98446ee67","cell_type":"code","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":"[debug] Encodings: locale UTF-8, fs utf-8, pref UTF-8, out UTF-8 (No ANSI), error UTF-8 (No ANSI), screen UTF-8 (No ANSI)\n[debug] yt-dlp version stable@2024.04.09 from yt-dlp/yt-dlp [ff0779267] (pip) API\n[debug] params: {'format': 'bestaudio/best', 'postprocessors': [{'key': 'FFmpegExtractAudio', 'preferredcodec': 'mp3', 'preferredquality': '192'}], 'outtmpl': 'files/audio/%(title)s.%(ext)s', 'verbose': True, 'compat_opts': set(), 'http_headers': {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.71 Safari/537.36', 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8', 'Accept-Language': 'en-us,en;q=0.5', 'Sec-Fetch-Mode': 'navigate'}}\n[debug] Python 3.8.10 (CPython x86_64 64bit) - Linux-5.10.234-225.921.amzn2.x86_64-x86_64-with-glibc2.29 (OpenSSL 1.1.1f  31 Mar 2020, glibc 2.31)\n[debug] exe versions: ffmpeg 4.2.7, ffprobe 4.2.7\n[debug] Optional libraries: Cryptodome-3.23.0, brotli-1.1.0, certifi-2019.11.28, mutagen-1.47.0, requests-2.31.0, secretstorage-3.3.3, sqlite3-3.31.1, urllib3-1.25.8, websockets-13.1\n[debug] Proxy map: {}\n[debug] Request Handlers: urllib, websockets\n[debug] Loaded 1810 extractors\n"},{"output_type":"stream","name":"stdout","text":"Downloading video from https://www.youtube.com/watch?v=aqzxYofJ_ck\n[youtube] Extracting URL: https://www.youtube.com/watch?v=aqzxYofJ_ck\n[youtube] aqzxYofJ_ck: Downloading webpage\n[youtube] aqzxYofJ_ck: Downloading ios player API JSON\n[youtube] aqzxYofJ_ck: Downloading android player API JSON\n"},{"output_type":"stream","name":"stderr","text":"ERROR: [youtube] aqzxYofJ_ck: Sign in to confirm you’re not a bot. This helps protect our community. Learn more\n  File \"/home/repl/.local/lib/python3.8/site-packages/yt_dlp/extractor/common.py\", line 734, in extract\n    ie_result = self._real_extract(url)\n  File \"/home/repl/.local/lib/python3.8/site-packages/yt_dlp/extractor/youtube.py\", line 4182, in _real_extract\n    self.raise_no_formats(reason, expected=True)\n  File \"/home/repl/.local/lib/python3.8/site-packages/yt_dlp/extractor/common.py\", line 1251, in raise_no_formats\n    raise ExtractorError(msg, expected=expected, video_id=video_id)\n\n[debug] Encodings: locale UTF-8, fs utf-8, pref UTF-8, out UTF-8 (No ANSI), error UTF-8 (No ANSI), screen UTF-8 (No ANSI)\n[debug] yt-dlp version stable@2024.04.09 from yt-dlp/yt-dlp [ff0779267] (pip) API\n[debug] params: {'format': 'bestaudio/best', 'postprocessors': [{'key': 'FFmpegExtractAudio', 'preferredcodec': 'mp3', 'preferredquality': '192'}], 'outtmpl': {'default': 'files/audio/%(title)s.%(ext)s', 'chapter': '%(title)s - %(section_number)03d %(section_title)s [%(id)s].%(ext)s'}, 'verbose': True, 'compat_opts': set(), 'http_headers': {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.71 Safari/537.36', 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8', 'Accept-Language': 'en-us,en;q=0.5', 'Sec-Fetch-Mode': 'navigate'}, 'forceprint': {}, 'print_to_file': {}}\n[debug] Python 3.8.10 (CPython x86_64 64bit) - Linux-5.10.234-225.921.amzn2.x86_64-x86_64-with-glibc2.29 (OpenSSL 1.1.1f  31 Mar 2020, glibc 2.31)\n[debug] exe versions: ffmpeg 4.2.7, ffprobe 4.2.7\n[debug] Optional libraries: Cryptodome-3.23.0, brotli-1.1.0, certifi-2019.11.28, mutagen-1.47.0, requests-2.31.0, secretstorage-3.3.3, sqlite3-3.31.1, urllib3-1.25.8, websockets-13.1\n[debug] Proxy map: {}\n[debug] Request Handlers: urllib, websockets\n[debug] Loaded 1810 extractors\n"},{"output_type":"stream","name":"stdout","text":"[youtube] Extracting URL: https://www.youtube.com/watch?v=aqzxYofJ_ck\n[youtube] aqzxYofJ_ck: Downloading webpage\n[youtube] aqzxYofJ_ck: Downloading ios player API JSON\n[youtube] aqzxYofJ_ck: Downloading android player API JSON\n"},{"output_type":"stream","name":"stderr","text":"ERROR: [youtube] aqzxYofJ_ck: Sign in to confirm you’re not a bot. This helps protect our community. Learn more\n  File \"/home/repl/.local/lib/python3.8/site-packages/yt_dlp/extractor/common.py\", line 734, in extract\n    ie_result = self._real_extract(url)\n  File \"/home/repl/.local/lib/python3.8/site-packages/yt_dlp/extractor/youtube.py\", line 4182, in _real_extract\n    self.raise_no_formats(reason, expected=True)\n  File \"/home/repl/.local/lib/python3.8/site-packages/yt_dlp/extractor/common.py\", line 1251, in raise_no_formats\n    raise ExtractorError(msg, expected=expected, video_id=video_id)\n\n"},{"output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mExtractorError\u001b[0m                            Traceback (most recent call last)","File \u001b[0;32m~/.local/lib/python3.8/site-packages/yt_dlp/YoutubeDL.py:1606\u001b[0m, in \u001b[0;36mYoutubeDL._handle_extraction_exceptions.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1605\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1606\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1607\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (DownloadCancelled, LazyList\u001b[38;5;241m.\u001b[39mIndexError, PagedList\u001b[38;5;241m.\u001b[39mIndexError):\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/yt_dlp/YoutubeDL.py:1741\u001b[0m, in \u001b[0;36mYoutubeDL.__extract_info\u001b[0;34m(self, url, ie, download, extra_info, process)\u001b[0m\n\u001b[1;32m   1740\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1741\u001b[0m     ie_result \u001b[38;5;241m=\u001b[39m \u001b[43mie\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m UserNotLive \u001b[38;5;28;01mas\u001b[39;00m e:\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/yt_dlp/extractor/common.py:734\u001b[0m, in \u001b[0;36mInfoExtractor.extract\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    732\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_screen(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExtracting URL: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m    733\u001b[0m     url \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_param(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mverbose\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m truncate_string(url, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m20\u001b[39m)))\n\u001b[0;32m--> 734\u001b[0m ie_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_real_extract\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ie_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/yt_dlp/extractor/youtube.py:4182\u001b[0m, in \u001b[0;36mYoutubeIE._real_extract\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m   4181\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m reason:\n\u001b[0;32m-> 4182\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_no_formats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreason\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   4184\u001b[0m keywords \u001b[38;5;241m=\u001b[39m get_first(video_details, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeywords\u001b[39m\u001b[38;5;124m'\u001b[39m, expected_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m []\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/yt_dlp/extractor/common.py:1251\u001b[0m, in \u001b[0;36mInfoExtractor.raise_no_formats\u001b[0;34m(self, msg, expected, video_id)\u001b[0m\n\u001b[1;32m   1250\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ExtractorError(msg, expected\u001b[38;5;241m=\u001b[39mexpected, video_id\u001b[38;5;241m=\u001b[39mvideo_id)\n","\u001b[0;31mExtractorError\u001b[0m: [youtube] aqzxYofJ_ck: Sign in to confirm you’re not a bot. This helps protect our community. Learn more","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mDownloadError\u001b[0m                             Traceback (most recent call last)","Cell \u001b[0;32mIn[11], line 12\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m youtube_dl\u001b[38;5;241m.\u001b[39mYoutubeDL(ydl_config) \u001b[38;5;28;01mas\u001b[39;00m ydl:\n\u001b[0;32m---> 12\u001b[0m         \u001b[43mydl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43myoutube_url\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m DownloadError:\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/yt_dlp/YoutubeDL.py:3572\u001b[0m, in \u001b[0;36mYoutubeDL.download\u001b[0;34m(self, url_list)\u001b[0m\n\u001b[1;32m   3571\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m url_list:\n\u001b[0;32m-> 3572\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__download_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_info\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3573\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_generic_extractor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforce_generic_extractor\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3575\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_retcode\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/yt_dlp/YoutubeDL.py:3547\u001b[0m, in \u001b[0;36mYoutubeDL.__download_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   3546\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3547\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3548\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m UnavailableVideoError \u001b[38;5;28;01mas\u001b[39;00m e:\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/yt_dlp/YoutubeDL.py:1595\u001b[0m, in \u001b[0;36mYoutubeDL.extract_info\u001b[0;34m(self, url, download, ie_key, extra_info, process, force_generic_extractor)\u001b[0m\n\u001b[1;32m   1594\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1595\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__extract_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_info_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1596\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/yt_dlp/YoutubeDL.py:1624\u001b[0m, in \u001b[0;36mYoutubeDL._handle_extraction_exceptions.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1623\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ExtractorError \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# An error we somewhat expected\u001b[39;00m\n\u001b[0;32m-> 1624\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreport_error\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_traceback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1625\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/yt_dlp/YoutubeDL.py:1073\u001b[0m, in \u001b[0;36mYoutubeDL.report_error\u001b[0;34m(self, message, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;124;03mDo the same as trouble, but prefixes the message with 'ERROR:', colored\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;124;03min red if stderr is a tty file.\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m-> 1073\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrouble\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_format_err\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mERROR:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStyles\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mERROR\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmessage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/yt_dlp/YoutubeDL.py:1012\u001b[0m, in \u001b[0;36mYoutubeDL.trouble\u001b[0;34m(self, message, tb, is_error)\u001b[0m\n\u001b[1;32m   1011\u001b[0m         exc_info \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexc_info()\n\u001b[0;32m-> 1012\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DownloadError(message, exc_info)\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_retcode \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n","\u001b[0;31mDownloadError\u001b[0m: ERROR: [youtube] aqzxYofJ_ck: Sign in to confirm you’re not a bot. This helps protect our community. Learn more","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mExtractorError\u001b[0m                            Traceback (most recent call last)","File \u001b[0;32m~/.local/lib/python3.8/site-packages/yt_dlp/YoutubeDL.py:1606\u001b[0m, in \u001b[0;36mYoutubeDL._handle_extraction_exceptions.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1605\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1606\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1607\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (DownloadCancelled, LazyList\u001b[38;5;241m.\u001b[39mIndexError, PagedList\u001b[38;5;241m.\u001b[39mIndexError):\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/yt_dlp/YoutubeDL.py:1741\u001b[0m, in \u001b[0;36mYoutubeDL.__extract_info\u001b[0;34m(self, url, ie, download, extra_info, process)\u001b[0m\n\u001b[1;32m   1740\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1741\u001b[0m     ie_result \u001b[38;5;241m=\u001b[39m \u001b[43mie\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m UserNotLive \u001b[38;5;28;01mas\u001b[39;00m e:\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/yt_dlp/extractor/common.py:734\u001b[0m, in \u001b[0;36mInfoExtractor.extract\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    732\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_screen(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExtracting URL: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m    733\u001b[0m     url \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_param(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mverbose\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m truncate_string(url, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m20\u001b[39m)))\n\u001b[0;32m--> 734\u001b[0m ie_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_real_extract\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ie_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/yt_dlp/extractor/youtube.py:4182\u001b[0m, in \u001b[0;36mYoutubeIE._real_extract\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m   4181\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m reason:\n\u001b[0;32m-> 4182\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_no_formats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreason\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   4184\u001b[0m keywords \u001b[38;5;241m=\u001b[39m get_first(video_details, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeywords\u001b[39m\u001b[38;5;124m'\u001b[39m, expected_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m []\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/yt_dlp/extractor/common.py:1251\u001b[0m, in \u001b[0;36mInfoExtractor.raise_no_formats\u001b[0;34m(self, msg, expected, video_id)\u001b[0m\n\u001b[1;32m   1250\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ExtractorError(msg, expected\u001b[38;5;241m=\u001b[39mexpected, video_id\u001b[38;5;241m=\u001b[39mvideo_id)\n","\u001b[0;31mExtractorError\u001b[0m: [youtube] aqzxYofJ_ck: Sign in to confirm you’re not a bot. This helps protect our community. Learn more","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mDownloadError\u001b[0m                             Traceback (most recent call last)","Cell \u001b[0;32mIn[11], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m DownloadError:\n\u001b[1;32m     14\u001b[0m      \u001b[38;5;28;01mwith\u001b[39;00m youtube_dl\u001b[38;5;241m.\u001b[39mYoutubeDL(ydl_config) \u001b[38;5;28;01mas\u001b[39;00m ydl: \n\u001b[0;32m---> 15\u001b[0m         \u001b[43mydl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43myoutube_url\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/yt_dlp/YoutubeDL.py:3572\u001b[0m, in \u001b[0;36mYoutubeDL.download\u001b[0;34m(self, url_list)\u001b[0m\n\u001b[1;32m   3569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SameFileError(outtmpl)\n\u001b[1;32m   3571\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m url_list:\n\u001b[0;32m-> 3572\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__download_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_info\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3573\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_generic_extractor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforce_generic_extractor\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3575\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_retcode\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/yt_dlp/YoutubeDL.py:3547\u001b[0m, in \u001b[0;36mYoutubeDL.__download_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   3544\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m   3545\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   3546\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3547\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3548\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m UnavailableVideoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   3549\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreport_error(e)\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/yt_dlp/YoutubeDL.py:1595\u001b[0m, in \u001b[0;36mYoutubeDL.extract_info\u001b[0;34m(self, url, download, ie_key, extra_info, process, force_generic_extractor)\u001b[0m\n\u001b[1;32m   1593\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m ExistingVideoReached()\n\u001b[1;32m   1594\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1595\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__extract_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_info_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1596\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1597\u001b[0m     extractors_restricted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mallowed_extractors\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m])\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/yt_dlp/YoutubeDL.py:1624\u001b[0m, in \u001b[0;36mYoutubeDL._handle_extraction_exceptions.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreport_error(msg)\n\u001b[1;32m   1623\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ExtractorError \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# An error we somewhat expected\u001b[39;00m\n\u001b[0;32m-> 1624\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreport_error\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_traceback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1625\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1626\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignoreerrors\u001b[39m\u001b[38;5;124m'\u001b[39m):\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/yt_dlp/YoutubeDL.py:1073\u001b[0m, in \u001b[0;36mYoutubeDL.report_error\u001b[0;34m(self, message, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreport_error\u001b[39m(\u001b[38;5;28mself\u001b[39m, message, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1069\u001b[0m     \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;124;03m    Do the same as trouble, but prefixes the message with 'ERROR:', colored\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;124;03m    in red if stderr is a tty file.\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m-> 1073\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrouble\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_format_err\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mERROR:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStyles\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mERROR\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmessage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/yt_dlp/YoutubeDL.py:1012\u001b[0m, in \u001b[0;36mYoutubeDL.trouble\u001b[0;34m(self, message, tb, is_error)\u001b[0m\n\u001b[1;32m   1010\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1011\u001b[0m         exc_info \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexc_info()\n\u001b[0;32m-> 1012\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DownloadError(message, exc_info)\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_retcode \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n","\u001b[0;31mDownloadError\u001b[0m: ERROR: [youtube] aqzxYofJ_ck: Sign in to confirm you’re not a bot. This helps protect our community. Learn more"],"ename":"DownloadError","evalue":"ERROR: [youtube] aqzxYofJ_ck: Sign in to confirm you’re not a bot. This helps protect our community. Learn more"}]},{"source":"To find the audio files that we will use the `glob`module that looks in the `output_dir` to find any .mp3 files. Then we will append the file to a list called `audio_files`. This will be used later to send each file to the Whisper model for transcription. ","metadata":{},"id":"df9c586d-309a-411b-90a5-6e81fe85eda4","cell_type":"markdown"},{"source":"### Instructions\n\nFind the audio file in the output directory.\n\n- Find all the MP3 audio files in the output directory by joining the output directory to the pattern `*.mp3` and using glob to list them.\n- Select the first file in the list and assign it to `audio_filename`.\n- _Check your work._ Print `audio_filename`.","metadata":{},"id":"0fa69a42-7065-4c3f-8699-fe48908f11b1","cell_type":"markdown"},{"source":"# Find the audio file in the output directory\n\n# Find all the audio files in the output directory\naudio=glob.glob(os.path.join(output_dir,\"*.mp3\"))\n\n# Select the first audio file in the list\naudio_filename=audio[0]\n\n# Print the name of the selected audio file\nprint(audio_filename)","metadata":{"executionCancelledAt":null,"executionTime":56,"lastExecutedAt":1747714097796,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Find the audio file in the output directory\n\n# Find all the audio files in the output directory\naudio=glob.glob(os.path.join(output_dir,\"*.mp3\"))\n\n# Select the first audio file in the list\naudio_filename=audio[0]\n\n# Print the name of the selected audio file\nprint(audio_filename)","outputsMetadata":{"0":{"height":38,"type":"stream"}},"lastExecutedByKernel":"71484f49-fabc-44a9-804f-e00db453715b"},"id":"c3d0a34d-ade9-4314-bc7d-480f165b3992","cell_type":"code","execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":"files/audio/Databytes.mp3\n"}]},{"source":"## Task 3: Transcribe the Video using Whisper","metadata":{},"id":"a9fe2a4e-b6ac-43d3-9b22-7df437015913","cell_type":"markdown"},{"source":"In this step we will take the downloaded and converted Youtube video and send it to the Whisper model to be transcribed. To do this we will create variables for the `audio_file`, for the `output_file` and the model. \n\nUsing these variables we will:\n- create a list to store the transcripts\n- Read the Audio File \n- Send the file to the Whisper Model using the OpenAI package ","metadata":{},"id":"1a00b32c-06e2-4fb1-8830-3634b13d133a","cell_type":"markdown"},{"source":"### Instructions\n\nTranscribe the audio file.\n\n- _The audio file, output file, and model are definied for you._\n- Define an OpenAI client model. Assign to client.\n- Open the audio file as read-binary (`\"rb\"`).\n  - Use the Whisper model to create a transcription of the opened audio file. Assign to `response`.\n- Extract the transcript from the response.","metadata":{},"id":"e4b60c5a-ea58-469e-b699-d46ef1cc7485","cell_type":"markdown"},{"source":"<details>\n<summary>Code hints</summary>\n<p>\n \nDefine the client model with `openai.OpenAI()`.\n\n---\n\nThe code pattern for opening a file in read-binary mode is.\n    \n```py\nwith open(file, \"rb\") as file_handle:\n    # do something\n```\n\n---\n    \nTo use the OpenAI API to create transcription, call the client's `.audio.transcriptions.create()` method, passing the file and the model.\n\n```py\nresponse = client.audio.transcriptions.create(file=audio, model=model)\n```\n\n---\n    \nThe transcript text is in the `.text` element of the response.\n    \n</p>\n</details>","metadata":{},"id":"1b3137d7-4b28-4da3-aea3-852e6345579f","cell_type":"markdown"},{"source":"# Use these settings\naudio_file = audio_filename\noutput_file = \"files/transcripts/transcript.txt\"\nmodel = \"whisper-1\"\n\n# Transcribe the audio file to text using OpenAI API\nprint(\"converting audio to text...\")\n\n# Define an OpenAI client model. Assign to client.\nclient = openai.OpenAI()\n\n# Open the audio file as read-binary\nwith open(audio_file,\"rb\") as f:\n    # Use the model to create a transcription\n    resp = client.audio.transcriptions.create(file = f , model = model)\n\n# Extract the transcript from the response\ntrans=resp.text\n\n# Print the transcript\nprint(trans)","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":613,"type":"stream"}},"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"lastExecutedByKernel":null},"id":"54306dcc-40f7-4a12-97ef-388b95c70ad4","cell_type":"code","execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":"converting audio to text...\nHi, in this tutorial, we're going to look at a data pre-processing technique for machine learning called splitting your data. That is splitting your data set into a training set and a testing set. Now, before we get to the code, you might wonder, why do I need to do this? And really, there are going to be two problems if you don't. So if you train your machine learning model on your whole data set, then you've not tested the model on anything else. And that means you don't know how well your model is going to perform on other data sets. Secondly, it's actually even worse than this, because you risk overfitting the model. And that means that you've made your model work really well for one data set, but that gives a cost of model performance on other data sets. So not only do you not know how well the model is going to perform on other data sets, it's probably going to be worse than it could be. So you might also wonder when in your machine learning workflow, as you're writing these different types of code, when does this come? So what's the point when you need to split your data set? And it's normally the last thing you do before feature engineering. So if you do this after feature engineering, then you risk having a problem called data leakage. And that means information from the testing set is going to be available in the training set, which is a form of cheating because it's going to make your model appear to perform better than it actually does. So it's giving you a sort of false sense of security. So if you find yourself doing feature engineering and you've not yet split your data into training and testing sets, then you need to back up a step. We're going to take a look at some loan application data. So I'm using DataCamp Workspace here, and this is one of the data sets that is available as standard with DataCamp Workspace. So there is a workspace template available if you want to try doing your own analysis on this data set. So because this data is in CSV format, I'm going to import the pandas package as pd, that's the sort of standard alias for it. And then we actually we just one function from scikit-learn. So this is in the scikit-learn model selection sub-module. And the function for splitting into training and testing sets is called train-test-split. So let's run that. All right, so this data is about loan applications. So I'm just going to call it loan applications. And we can use pd.read.csv because it is in a CSV file. And the file is called loan underscore data dot csv. Let me just check and see if I got that correct. Loan underscore data dot csv. Yes, it did. Okay, so let me just copy and paste this variable name so we can print out the results. Okay, so here you can see the table here. Actually, to make this easier, we've got nine and a half thousand rows here. What I'm going to do is I'm just going to import the first thousand rows. And this is going to make some of the results a bit easier to understand. All right, so now we've only got a thousand rows of data. You can see we've got this column called credit policy. This is going to be our response variable. And then we've got a load of other variables we can use as features. This purpose column, because it's a categorical column, that's going to become important how we deal with that in a moment. All right, so first of all, we'll just concentrate on the response. So the response variable is called credit dot policy. And so each row is an application. So when that application meets the underwriting policy, so it meets the kind of loan criteria, it takes the value one, and it's a zero if the application was not up to scratch. So I'm going to call this variable response. Some people like to call the response variable just lowercase y. I think response is a bit more meaningful, particularly in this case. So we're going to start off with the loan applications data frame. And we're going to take the credit policy column, and then I'm going to copy and paste this variable name again, so you can see the results. So in this case, it is a Pandas series, and you can see it's got ones and zeros. All right, so we're going to use all the other columns for features. So again, let's call this variable features. Some people like to use capital X for this. So again, I'm going to start with loan applications with a T somewhere in there. And we use every column except credit policy. So this drop method is a little shortcut for just like saying, I want everything in the data frame except a specific set of columns. Now, one extra little trick. So as I mentioned, we have this categorical variable called purpose. So we need one hot encoding on this in order to turn it into a series of numeric columns with zeros and ones. So we can use pd.get dummies for that. Let's code another line so it's easy to see how it breaks down. And then I'm going to print this out. So, so far, this is pretty standard code just for splitting your data set into a response variable and some features. So here we've got 19 columns now. So the one important thing to note is that purpose column is now several different columns with ones and zeros. All right, so now the crux of this. So we're going to split the response and the features up into training sets and testing sets. So we're going to call the train test split function and pass those two variables in. So we're going to call train test split. I'm going to pass it the response first and then the features. So let's run this and you can see the output. It's a little bit squiffy. So what we get is a list and it actually has four different things in it. So we get the responses and the features for the training set and the responses and features for the testing set. And actually the trick is just remembering which order they're returned in. So rather than returning a list, it's slightly easier if we use variable unpacking and we return four different objects together. So we're going to return four things from this one function call. So I'm going to call this response train and I think the same one as a response test and then we go features train and features test. So we've got four different variables here and let me run that. So I can sort of print these out one at a time but it's not that exciting because you've seen the whole data set before, it's just bits of it. So what's actually slightly more useful is if we take a look at the shape of each of these and you can see how much data has ended up in each one. So I'm going to print this out and we're going to do the same with the test set and features test. All right, so we've got four different variables here. Now you can see we started off with a thousand rows and so the responses, these are series, they don't have any columns, but you can see that 75% of them, so 750 out of a thousand, have ended up in the training set and 250, I'll just highlight that, have ended up in the testing set and it's the same with the features as well. So we've got 750 in the training set and 250 in the test set. You've got 19 columns here 19 features. So by default 75% of our data has ended up in the training set, 25% has ended up in the testing set and normally this is like perfectly fine. Sometimes if you have like a small data set you might want a little bit more in the training set and a little bit less in the testing set and if you've got a very large data set then you might say well okay I want 70% of data in the training set and 30% in the testing set. So in this case, because we've only got a thousand rows, let's shrink the the testing set a little bit. So we're going to do the same again but we're going to use the test size argument to get a smaller testing set. So I'm just going to copy and paste this code, run typing again. So the change here, we're going to add an additional argument called test size, I'm going to set that to 0.2. So we're going to have 80% in the training set, 20% in the test set. Let's run that and again I am going to copy and paste this code that shows the shapes of the output and here you can see now we've got 800 of the thousand rows in the training set, 200 in the testing set. Now one more thing you might be interested in doing is reproducing the values that are provided in both training testing sets. What I mean by that is that by default the training and testing sets are randomly generated. So each of the rows from the data set is randomly allocated to one or other of these sets. If you're writing a report you might want to have your results exactly the same every time. Another case where this is useful is if you're trying to find a problem with your model then you might want to be able to demonstrate the problem precisely to someone else. So sometimes you want your code to be exactly reproducible despite the fact you've got random things in it and for this work you need to set a random seed and you can use the random state argument to do this. So what I'm going to do is I'm going to run this code twice but we're going to set the random state argument so you can make any number you like. I'm just going to pick 999 and so we because we set the random state this code is going to run the same thing twice. Let me give different variable names for what's being returned here. So I'm going to run this so we've done the split twice and so let's have a look at one of these. So if you have a look at features train you can see the values you've got row 46, row 748 and so on and then let's add another one of these. So we're going to look at features train 2 and you see even though it's random we have exactly the same results. It's row 46, row 748, row 524 and so on. So exactly the same result in both cases and that's more or less all there is to splitting your data into training and testing sets. I hope it's been helpful.\n"}]},{"source":"### Instructions\n\nSave the transcript to a text file.\n\n- If the directory for the output file doesn't exist, make it.\n- Write the transcript to the output file","metadata":{},"id":"0b7b4b2d-6e2b-4f86-b8b5-c58725c8de08","cell_type":"markdown"},{"source":"<details>\n<summary>Code hints</summary>\n<p>\n \n- Get the directory to write to with `os.path.dirname()` and the output file.\n- Make a directory with `os.makedirs()`. Set `exist_ok` to `True` to prevent errors if the directory already exists.\n\n---\n\nThe code pattern for writing a text file is as follows.\n    \n```py\nwith open(filename, \"w\") as file:\n    file.write(text)\n```\n\n\n</p>\n</details>","metadata":{},"id":"b30c40b2-ed8f-479c-a6ea-1b9f22359991","cell_type":"markdown"},{"source":"# Create the directory for the output file if it doesn't exist\nos.makedirs(os.path.dirname(output_file),exist_ok=True)\n# Write the transcript to the output file\nwith open(output_file,\"w\") as file:\n    file.write(trans)\n\n","metadata":{"executionCancelledAt":null,"executionTime":30,"lastExecutedAt":1747714536901,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Create the directory for the output file if it doesn't exist\nos.makedirs(os.path.dirname(output_file),exist_ok=True)\n# Write the transcript to the output file\nwith open(output_file,\"w\") as file:\n    file.write(trans)\n\n","outputsMetadata":{"0":{"height":532,"type":"stream"}},"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"lastExecutedByKernel":"71484f49-fabc-44a9-804f-e00db453715b"},"id":"6798bff4-ac8d-46e3-8c62-e540655e859d","cell_type":"code","execution_count":24,"outputs":[]},{"source":"## Task 4: Create a TextLoader using LangChain ","metadata":{},"id":"c1001454-eb29-4981-825f-fa08e2fc48e8","cell_type":"markdown"},{"source":"In order to use text or other types of data with LangChain we must first convert that data into Documents. This is done by using loaders. In this tutorial, we will use the `TextLoader` that will take the text from our transcript and load it into a document. ","metadata":{},"id":"8191715b-72ad-4a34-ac95-02e1fbf8d391","cell_type":"markdown"},{"source":"### Instructions\n\nLoad the documents from the text file using a TextLoader.\n\n- From the `langchain.document_loaders` module, import `TextLoader`.\n- Create a `TextLoader`, passing it the directory of the transcripts, `\"./files/text\"`. Assign to `loader`.\n- Use the TextLoader to load the documents. Assign to `docs`.","metadata":{},"id":"4f75f541-5bd7-4214-a75e-79681303c6f6","cell_type":"markdown"},{"source":"# From the langchain.document_loaders module, import TextLoader\nfrom langchain.document_loaders import TextLoader\n\n# Create a `TextLoader`, passing the directory of the transcripts. Assign to `loader`.\nloader=TextLoader(\"./files/transcripts/transcript.txt\")\n\n# Use the TextLoader to load the documents. Assign to docs.\ndocs=loader.load()","metadata":{"executionCancelledAt":null,"executionTime":15,"lastExecutedAt":1747716542248,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# From the langchain.document_loaders module, import TextLoader\nfrom langchain.document_loaders import TextLoader\n\n# Create a `TextLoader`, passing the directory of the transcripts. Assign to `loader`.\nloader=TextLoader(\"./files/transcripts/transcript.txt\")\n\n# Use the TextLoader to load the documents. Assign to docs.\ndocs=loader.load()","lastExecutedByKernel":"71484f49-fabc-44a9-804f-e00db453715b"},"id":"bb8654f7-965e-4e62-98ab-d08b7026e3d9","cell_type":"code","execution_count":27,"outputs":[]},{"source":"# Show the first element of docs to verify it has been loaded \ndocs[0]","metadata":{"executionCancelledAt":null,"executionTime":12,"lastExecutedAt":1747716576892,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Show the first element of docs to verify it has been loaded \ndocs[0]","collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"lastExecutedByKernel":"71484f49-fabc-44a9-804f-e00db453715b"},"id":"269aaed5-7d07-43d7-a2d0-a89730ec4bc9","cell_type":"code","execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":"Document(page_content=\"Hi, in this tutorial, we're going to look at a data pre-processing technique for machine learning called splitting your data. That is splitting your data set into a training set and a testing set. Now, before we get to the code, you might wonder, why do I need to do this? And really, there are going to be two problems if you don't. So if you train your machine learning model on your whole data set, then you've not tested the model on anything else. And that means you don't know how well your model is going to perform on other data sets. Secondly, it's actually even worse than this, because you risk overfitting the model. And that means that you've made your model work really well for one data set, but that gives a cost of model performance on other data sets. So not only do you not know how well the model is going to perform on other data sets, it's probably going to be worse than it could be. So you might also wonder when in your machine learning workflow, as you're writing these different types of code, when does this come? So what's the point when you need to split your data set? And it's normally the last thing you do before feature engineering. So if you do this after feature engineering, then you risk having a problem called data leakage. And that means information from the testing set is going to be available in the training set, which is a form of cheating because it's going to make your model appear to perform better than it actually does. So it's giving you a sort of false sense of security. So if you find yourself doing feature engineering and you've not yet split your data into training and testing sets, then you need to back up a step. We're going to take a look at some loan application data. So I'm using DataCamp Workspace here, and this is one of the data sets that is available as standard with DataCamp Workspace. So there is a workspace template available if you want to try doing your own analysis on this data set. So because this data is in CSV format, I'm going to import the pandas package as pd, that's the sort of standard alias for it. And then we actually we just one function from scikit-learn. So this is in the scikit-learn model selection sub-module. And the function for splitting into training and testing sets is called train-test-split. So let's run that. All right, so this data is about loan applications. So I'm just going to call it loan applications. And we can use pd.read.csv because it is in a CSV file. And the file is called loan underscore data dot csv. Let me just check and see if I got that correct. Loan underscore data dot csv. Yes, it did. Okay, so let me just copy and paste this variable name so we can print out the results. Okay, so here you can see the table here. Actually, to make this easier, we've got nine and a half thousand rows here. What I'm going to do is I'm just going to import the first thousand rows. And this is going to make some of the results a bit easier to understand. All right, so now we've only got a thousand rows of data. You can see we've got this column called credit policy. This is going to be our response variable. And then we've got a load of other variables we can use as features. This purpose column, because it's a categorical column, that's going to become important how we deal with that in a moment. All right, so first of all, we'll just concentrate on the response. So the response variable is called credit dot policy. And so each row is an application. So when that application meets the underwriting policy, so it meets the kind of loan criteria, it takes the value one, and it's a zero if the application was not up to scratch. So I'm going to call this variable response. Some people like to call the response variable just lowercase y. I think response is a bit more meaningful, particularly in this case. So we're going to start off with the loan applications data frame. And we're going to take the credit policy column, and then I'm going to copy and paste this variable name again, so you can see the results. So in this case, it is a Pandas series, and you can see it's got ones and zeros. All right, so we're going to use all the other columns for features. So again, let's call this variable features. Some people like to use capital X for this. So again, I'm going to start with loan applications with a T somewhere in there. And we use every column except credit policy. So this drop method is a little shortcut for just like saying, I want everything in the data frame except a specific set of columns. Now, one extra little trick. So as I mentioned, we have this categorical variable called purpose. So we need one hot encoding on this in order to turn it into a series of numeric columns with zeros and ones. So we can use pd.get dummies for that. Let's code another line so it's easy to see how it breaks down. And then I'm going to print this out. So, so far, this is pretty standard code just for splitting your data set into a response variable and some features. So here we've got 19 columns now. So the one important thing to note is that purpose column is now several different columns with ones and zeros. All right, so now the crux of this. So we're going to split the response and the features up into training sets and testing sets. So we're going to call the train test split function and pass those two variables in. So we're going to call train test split. I'm going to pass it the response first and then the features. So let's run this and you can see the output. It's a little bit squiffy. So what we get is a list and it actually has four different things in it. So we get the responses and the features for the training set and the responses and features for the testing set. And actually the trick is just remembering which order they're returned in. So rather than returning a list, it's slightly easier if we use variable unpacking and we return four different objects together. So we're going to return four things from this one function call. So I'm going to call this response train and I think the same one as a response test and then we go features train and features test. So we've got four different variables here and let me run that. So I can sort of print these out one at a time but it's not that exciting because you've seen the whole data set before, it's just bits of it. So what's actually slightly more useful is if we take a look at the shape of each of these and you can see how much data has ended up in each one. So I'm going to print this out and we're going to do the same with the test set and features test. All right, so we've got four different variables here. Now you can see we started off with a thousand rows and so the responses, these are series, they don't have any columns, but you can see that 75% of them, so 750 out of a thousand, have ended up in the training set and 250, I'll just highlight that, have ended up in the testing set and it's the same with the features as well. So we've got 750 in the training set and 250 in the test set. You've got 19 columns here 19 features. So by default 75% of our data has ended up in the training set, 25% has ended up in the testing set and normally this is like perfectly fine. Sometimes if you have like a small data set you might want a little bit more in the training set and a little bit less in the testing set and if you've got a very large data set then you might say well okay I want 70% of data in the training set and 30% in the testing set. So in this case, because we've only got a thousand rows, let's shrink the the testing set a little bit. So we're going to do the same again but we're going to use the test size argument to get a smaller testing set. So I'm just going to copy and paste this code, run typing again. So the change here, we're going to add an additional argument called test size, I'm going to set that to 0.2. So we're going to have 80% in the training set, 20% in the test set. Let's run that and again I am going to copy and paste this code that shows the shapes of the output and here you can see now we've got 800 of the thousand rows in the training set, 200 in the testing set. Now one more thing you might be interested in doing is reproducing the values that are provided in both training testing sets. What I mean by that is that by default the training and testing sets are randomly generated. So each of the rows from the data set is randomly allocated to one or other of these sets. If you're writing a report you might want to have your results exactly the same every time. Another case where this is useful is if you're trying to find a problem with your model then you might want to be able to demonstrate the problem precisely to someone else. So sometimes you want your code to be exactly reproducible despite the fact you've got random things in it and for this work you need to set a random seed and you can use the random state argument to do this. So what I'm going to do is I'm going to run this code twice but we're going to set the random state argument so you can make any number you like. I'm just going to pick 999 and so we because we set the random state this code is going to run the same thing twice. Let me give different variable names for what's being returned here. So I'm going to run this so we've done the split twice and so let's have a look at one of these. So if you have a look at features train you can see the values you've got row 46, row 748 and so on and then let's add another one of these. So we're going to look at features train 2 and you see even though it's random we have exactly the same results. It's row 46, row 748, row 524 and so on. So exactly the same result in both cases and that's more or less all there is to splitting your data into training and testing sets. I hope it's been helpful.\", metadata={'source': './files/transcripts/transcript.txt'})"},"metadata":{},"execution_count":28}]},{"source":"## Task 5: Create an In-Memory Vector Store ","metadata":{},"id":"577069b3-02f6-4b73-aaaa-d8b8e8006d98","cell_type":"markdown"},{"source":"Now that we have created Documents of the transcription, we will store that Document in a vector store. Vector stores allows LLMs to traverse through data to find similiarity between different data based on their distance in space. \n\nFor large amounts of data, it is best to use a designated Vector Database. Since we are only using one transcript for this tutorial, we can create an in-memory vector store using the `docarray` package. \n\nWe will also tokenize our queries using the `tiktoken` package. This means that our query will be seperated into smaller parts either by phrases, words or characters. Each of these parts are assigned a token which helps the model \"understand\" the text and relationships with other tokens. ","metadata":{},"id":"79af9e43-c32f-478d-b057-dc3b7890925e","cell_type":"markdown"},{"source":"### Instructions\n\n- Import the `tiktoken` package. ","metadata":{},"id":"d3a5eb22-3a34-40a5-9f00-bd4895a1c4ca","cell_type":"markdown"},{"source":"# Import the tiktoken package\nimport tiktoken","metadata":{"executionCancelledAt":null,"executionTime":54,"lastExecutedAt":1747716612450,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import the tiktoken package\nimport tiktoken","lastExecutedByKernel":"71484f49-fabc-44a9-804f-e00db453715b"},"id":"15298bd3-5465-450d-b917-5e5d87d78bf2","cell_type":"code","execution_count":29,"outputs":[]},{"source":"## Task 6: Create the Document Search ","metadata":{},"id":"6e01af9c-f5ea-4382-b7ff-01fe0bb30edc","cell_type":"markdown"},{"source":"We will now use LangChain to complete some important operations to create the Question and Answer experience. Let´s import the follwing: \n\n- Import `RetrievalQA` from `langchain.chains` - this chain first retrieves documents from an assigned Retriver and then runs a QA chain for answering over those documents \n- Import `ChatOpenAI` from `langchain.chat_models` - this imports the ChatOpenAI model that we will use to query the data \n- Import `DocArrayInMemorySearch` from `langchain.vectorstores` - this gives the ability to search over the vector store we have created. \n- Import `OpenAIEmbeddings` from `langchain.embeddings` - this will create embeddings for the data store in the vector store. \n- Import `display` and `Markdown`from `IPython.display` - this will create formatted responses to the queries. (","metadata":{},"id":"22438b44-b8f8-4c78-a573-87ee4bdb2234","cell_type":"markdown"},{"source":"# From the langchain.chains module, import RetrievalQA\nfrom langchain.chains import RetrievalQA\n\n# From the langchain_openai package, import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\n\n# From the langchain.vectorstores module, import DocArrayInMemorySearch\nfrom langchain.vectorstores import DocArrayInMemorySearch","metadata":{"executionCancelledAt":null,"executionTime":11,"lastExecutedAt":1747716916487,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# From the langchain.chains module, import RetrievalQA\nfrom langchain.chains import RetrievalQA\n\n# From the langchain_openai package, import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\n\n# From the langchain.vectorstores module, import DocArrayInMemorySearch\nfrom langchain.vectorstores import DocArrayInMemorySearch","lastExecutedByKernel":"71484f49-fabc-44a9-804f-e00db453715b"},"id":"3a7fb40d-de20-4ec8-b05a-036b6dc6ad66","cell_type":"code","execution_count":36,"outputs":[]},{"source":"Now we will create a vector store that will use the `DocArrayInMemory` search methods which will search through the created embeddings created by the OpenAI Embeddings function. ","metadata":{},"id":"9bec39d2-8a4c-4638-953f-fbd9fa47ad6f","cell_type":"markdown"},{"source":"### Instructions\n\nCreate an in-memory search object from the specified documents, `docs` and embeddings, `OpenAIEmbeddings()`. Assign to `db`.","metadata":{},"id":"665d55d7-25fb-4aeb-9434-6b76de0ee405","cell_type":"markdown"},{"source":"<details>\n<summary>Code hints</summary>\n<p>\n \n- `DocArrayInMemorySearch` has a `.from_documents()` method that takes two arguments: the documents and the embeddings.\n- The documents were created when you used the `TextLoader`.\n- `OpenAIEmbeddings()` specifies the type of embeddings to use.\n\n</p>\n</details>","metadata":{},"id":"5385cba2-7d13-4368-a9b3-8b76d57ffe6c","cell_type":"markdown"},{"source":"# Create a new DocArrayInMemorySearch instance from the specified documents and embeddings\ndb = DocArrayInMemorySearch.from_documents(docs,OpenAIEmbeddings())","metadata":{"executionCancelledAt":null,"executionTime":3982,"lastExecutedAt":1747716974695,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Create a new DocArrayInMemorySearch instance from the specified documents and embeddings\ndb = DocArrayInMemorySearch.from_documents(docs,OpenAIEmbeddings())","lastExecutedByKernel":"71484f49-fabc-44a9-804f-e00db453715b"},"id":"66ef212c-eefd-4cf2-a02c-3c01b1b29118","cell_type":"code","execution_count":37,"outputs":[]},{"source":"We will now create a retriever from the `db` we created in the last step. This enables the retrieval of the stored embeddings. Since we are also using the `ChatOpenAI` model, will assigned that as our LLM.\n\nRecall that the temperature of an LLM refers to how random the results are. Setting the temperature to zero makes the results more repeatable.","metadata":{},"id":"033f3ebc-c098-49e8-a96f-f428940996d9","cell_type":"markdown"},{"source":"### Instructions\n\n- Convert the `DocArrayInMemorySearch` instance to a retriever. Assign to `retriever`.\n- Create a new `ChatOpenAI` instance with a temperature of `0.0`. Assign to `llm`. ","metadata":{},"id":"0aabff95-8fa2-47ea-b0b3-23c53c6d3c38","cell_type":"markdown"},{"source":"<details>\n<summary>Code hints</summary>\n<p>\n \n- `DocArrayInMemorySearch` has a `.as_retriever()` method to convert it to a retriever object. No arguments are required.\n- Create a client model with `ChatOpenAI()`, setting the `temperature` argument.\n\n</p>\n</details>","metadata":{},"id":"f005ea57-b2fa-4d13-9957-4d23d9b01d15","cell_type":"markdown"},{"source":"# Convert the DocArrayInMemorySearch instance to a retriever\nretriever= db.as_retriever()\n\n# Create a new ChatOpenAI instance with a temperature of 0.0\nllm= ChatOpenAI(temperature=0.0)","metadata":{"executionCancelledAt":null,"executionTime":23,"lastExecutedAt":1747717306813,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Convert the DocArrayInMemorySearch instance to a retriever\nretriever= db.as_retriever()\n\n# Create a new ChatOpenAI instance with a temperature of 0.0\nllm= ChatOpenAI(temperature=0.0)","lastExecutedByKernel":"71484f49-fabc-44a9-804f-e00db453715b"},"id":"7c7f6113-c145-47ff-ab9c-ada04ca047ce","cell_type":"code","execution_count":38,"outputs":[]},{"source":"Our last step before starting to ask questions is to create the `RetrievalQA` chain. This chain takes in the:  \n- The `llm` we want to use.\n- The `chain_type` which is how the model retrieves the data. Here we will use a _stuff_ chain, where all the documents are stuffed into the prompt. It is the simplest type, but only works where you only have a few small documents.\n- The `retriever` that we have created.\n- An option called `verbose` that prints details of each step of the chain.","metadata":{},"id":"4f5a7c7a-1676-41e0-976a-50316a684d12","cell_type":"markdown"},{"source":"### Instructions\n\nCreate a new RetrievalQA instance from the chain type. \n\n- Set `llm` to the ChatOpenAI instance you just created.\n- Set `chain_type` to `\"stuff\"`.\n- Set `retriever` to the retriever you just created.\n- Set `verbose` to `True`.","metadata":{},"id":"2a5ce4f9-e025-40e6-b737-be77213d5110","cell_type":"markdown"},{"source":"<details>\n<summary>Code hints</summary>\n<p>\n \nThe code pattern to create a `RetrievalQA` instance from a chain type is as follows.\n    \n```py\nRetrievalQA.from_chain_type(\n    llm=your_model,\n    chain_type=your_chain_type,\n    retriever=your_retriever,\n    any_other_options\n)\n```\n\n</p>\n</details>","metadata":{},"id":"448bee81-92ea-4632-b250-d8f8ae90c64c","cell_type":"markdown"},{"source":"# Create a new RetrievalQA instance with the specified parameters\nqa_stuff= RetrievalQA.from_chain_type(\n    llm=llm,\n    chain_type=\"stuff\",\n    retriever= retriever,\n    verbose= True\n)\n    # The ChatOpenAI instance to use for generating responses\n    # The type of chain to use for the QA system\n    # The retriever to use for retrieving relevant documents\n    # Whether to print verbose output during retrieval and generation\n","metadata":{"executionCancelledAt":null,"executionTime":11,"lastExecutedAt":1747717427019,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Create a new RetrievalQA instance with the specified parameters\nqa_stuff= RetrievalQA.from_chain_type(\n    llm=llm,\n    chain_type=\"stuff\",\n    retriever= retriever,\n    verbose= True\n)\n    # The ChatOpenAI instance to use for generating responses\n    # The type of chain to use for the QA system\n    # The retriever to use for retrieving relevant documents\n    # Whether to print verbose output during retrieval and generation\n","lastExecutedByKernel":"71484f49-fabc-44a9-804f-e00db453715b"},"id":"09fc202b-198f-4510-8d81-258f914d5c08","cell_type":"code","execution_count":40,"outputs":[]},{"source":"## Task 7: Create the Queries ","metadata":{},"id":"4ce8cff5-ab49-44f0-96ee-88826d88ea6a","cell_type":"markdown"},{"source":"Now we are ready to create queries about the YouTube video and read the responses from the LLM. This done first by creating a query and then running the RetrievalQA we setup in the last step and passing it the query. ","metadata":{},"id":"d51218d4-4e81-4d87-9f3f-77eacde057c1","cell_type":"markdown"},{"source":"### Instructions\n\nAsk GPT some questions about the transcript.\n\n- Create question, `\"What is this tutorial about?\"`. Assign to `query`.\n- Invoke the query through the RetrievalQA instance. Assign to `response`. \n- Print the response.","metadata":{},"id":"5e2b036b-cef6-4b52-9421-5ccf2b865482","cell_type":"markdown"},{"source":"<details>\n<summary>Code hints</summary>\n<p>\n \nCall the `.invoke()` method of the RetrievalQA instance, passing the query. The code pattern is as follows.\n    \n```py\nresponse = qa.invoke(text)\n```\n\n</p>\n</details>","metadata":{},"id":"5dc914bb-dba3-4880-90bc-15cd037ce911","cell_type":"markdown"},{"source":"# Set the query to be used for the QA system\nquery=\"what is the tuorial about?\"\n\n# Invoke the query through the RetrievalQA instance. Assign to response.\nresponse = qa_stuff.run(query)\n\n# Print the response to the console\nresponse\n","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":122,"type":"stream"}},"lastExecutedByKernel":null},"id":"d576672c-5078-487a-9dc5-3703f17d82f1","cell_type":"code","execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":"\n\n\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n\n\u001b[1m> Finished chain.\u001b[0m\n"},{"output_type":"execute_result","data":{"text/plain":"\"The tutorial is about a data pre-processing technique for machine learning called splitting your data. It focuses on splitting a data set into a training set and a testing set to avoid overfitting and to ensure the model's performance on unseen data sets. The tutorial also covers the importance of when to split the data in the machine learning workflow and provides a practical example using loan application data.\""},"metadata":{},"execution_count":42}]},{"source":"We can continue on creating queries and even creating queries that we know would not be answered in this video to see how the model responds. ","metadata":{},"id":"de9f2df3-87d1-40d3-862a-95769c11d015","cell_type":"markdown"},{"source":"# Set the query to be used for the QA system\nquery = \"What is the difference between a training set and test set?\"\n\n# Invoke the query through the RetrievalQA instance and store the response\nresponse=qa_stuff.run(query)\n\n# Print the response to the console\nresponse","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":122,"type":"stream"}},"lastExecutedByKernel":null},"id":"dbb75225-76c1-4eb8-9055-e13a3bb68682","cell_type":"code","execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":"\n\n\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n\n\u001b[1m> Finished chain.\u001b[0m\n"},{"output_type":"execute_result","data":{"text/plain":"'The training set is used to train the machine learning model, meaning the model learns patterns and relationships within the data. The test set, on the other hand, is used to evaluate the performance of the trained model on unseen data. The training set helps the model learn, while the test set helps assess how well the model generalizes to new data.'"},"metadata":{},"execution_count":44}]},{"source":"# Set the query to be used for the QA system\nquery = \"Who should watch this lesson?\"\n\n# Invoke the query through the RetrievalQA instance and store the response\nresponse=qa_stuff.run(query)\n\n# Print the response to the console\nresponse","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":122,"type":"stream"}},"lastExecutedByKernel":null},"id":"13864a14-0eda-4afd-bfe5-90fdefbc5d49","cell_type":"code","execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":"\n\n\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n\n\u001b[1m> Finished chain.\u001b[0m\n"},{"output_type":"execute_result","data":{"text/plain":"'This lesson on splitting data into training and testing sets is beneficial for individuals who are learning or working with machine learning techniques. It provides essential knowledge on the importance of splitting data, the risks of not doing so, and the proper timing of this process in a machine learning workflow.'"},"metadata":{},"execution_count":45}]},{"source":"# Set the query to be used for the QA system\nquery = \"Who is the greatest football team on earth?\"\n\n# Invoke the query through the RetrievalQA instance and store the response\nresponse=qa_stuff.run(query)\n\n# Print the response to the console\nresponse","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":122,"type":"stream"}},"lastExecutedByKernel":null},"id":"c62b73e4-e746-49f9-8106-921cbb4e6df8","cell_type":"code","execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":"\n\n\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n\n\u001b[1m> Finished chain.\u001b[0m\n"},{"output_type":"execute_result","data":{"text/plain":"\"I don't know the answer to that question.\""},"metadata":{},"execution_count":46}]},{"source":"# Set the query to be used for the QA system\nquery = \"How long is the circumference of the earth?\"\n\n# Invoke the query through the RetrievalQA instance and store the response\nresponse=qa_stuff.run(query)\n\n# Print the response to the console\nresponse","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":122,"type":"stream"},"1":{"height":77,"type":"stream"}},"lastExecutedByKernel":null},"id":"f9f7a7f3-f0f1-44ad-be76-d15aa009ac34","cell_type":"code","execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":"\n\n\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n\n\u001b[1m> Finished chain.\u001b[0m\n"},{"output_type":"execute_result","data":{"text/plain":"\"I don't know the exact length of the circumference of the earth.\""},"metadata":{},"execution_count":47}]},{"source":"## All done, congrats! ","metadata":{},"id":"65454beb-970f-4af6-a04c-798b9f665b6f","cell_type":"markdown"}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}